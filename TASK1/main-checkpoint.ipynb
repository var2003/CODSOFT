{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697fa31a-28d7-4d29-b1a4-42a12cc3d6fe",
   "metadata": {},
   "source": [
    " # 1.Movie Genre Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8993d-df90-4bf4-8a72-0184bc81e966",
   "metadata": {},
   "source": [
    "# Movie Genre Classification\n",
    "\n",
    "This notebook walks through building a machine learning model to classify movie genres based on plot summaries.\n",
    "\n",
    "We'll:\n",
    "- Load and clean the data\n",
    "- Vectorize the text using TF-IDF\n",
    "- Train a Logistic Regression model with hyperparameter tuning\n",
    "- Evaluate the model\n",
    "- Predict new examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447e8f8-9559-4653-a1cd-f35e79bc52e3",
   "metadata": {},
   "source": [
    "# 2.Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dda0b8e-ed5c-402f-a7a9-c91a84ca17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VARSHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Setup logging for info\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize tokenizer and stopwords\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d6f43-1b54-4b86-91e7-4ab73fa3343a",
   "metadata": {},
   "source": [
    "# 3.Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d431e20c-399c-4d34-8633-2fbf1702f363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>plot_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Martian</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>An astronaut is stranded on Mars and must find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>Drama</td>\n",
       "      <td>A jury deliberates over the fate of a teenager...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Inception</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>A thief enters people's dreams to steal secret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>Romance</td>\n",
       "      <td>A young couple falls in love aboard the ill-fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Crime</td>\n",
       "      <td>An organized crime dynasty's aging patriarch t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id          title    genre  \\\n",
       "0   1    The Martian   Sci-Fi   \n",
       "1   2   12 Angry Men    Drama   \n",
       "2   3      Inception   Sci-Fi   \n",
       "3   4        Titanic  Romance   \n",
       "4   5  The Godfather    Crime   \n",
       "\n",
       "                                        plot_summary  \n",
       "0  An astronaut is stranded on Mars and must find...  \n",
       "1  A jury deliberates over the fate of a teenager...  \n",
       "2  A thief enters people's dreams to steal secret...  \n",
       "3  A young couple falls in love aboard the ill-fa...  \n",
       "4  An organized crime dynasty's aging patriarch t...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'movies.csv' with your actual file path if needed\n",
    "df = pd.read_csv('movies.csv')\n",
    "\n",
    "# Show first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354580d-7ca1-4ebe-9c27-96cb792ae9ed",
   "metadata": {},
   "source": [
    "# 4.Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e81dfbc-f2b3-485a-b242-61f30d2b516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))  # Remove non-alphabetic chars\n",
    "    text = text.lower()                           # Lowercase\n",
    "    tokens = tokenizer.tokenize(text)             # Tokenize using TreebankWordTokenizer\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785d9d7-3a7c-4e5c-8501-7f3c4f3b5580",
   "metadata": {},
   "source": [
    "# 5.Apply Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b26f39bd-a178-4a4b-831e-7466c80a9be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VARSHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VARSHA\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setup tqdm for pandas\n",
    "tqdm.pandas(desc=\"Cleaning plot summaries\")\n",
    "\n",
    "# Optional: configure stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Toke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc815e0-f6ca-4ecf-bcb8-75344a46751d",
   "metadata": {},
   "source": [
    "# 6.Filter Rare Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "416a1807-df18-4346-a156-41e94dbf70af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 09:37:46,271 - INFO - Genres after filtering:\n",
      "genre\n",
      "Sci-Fi    3\n",
      "Drama     2\n",
      "Action    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove genres with fewer than 2 samples\n",
    "genre_counts = df['genre'].value_counts()\n",
    "valid_genres = genre_counts[genre_counts >= 2].index\n",
    "df_filtered = df[df['genre'].isin(valid_genres)]\n",
    "\n",
    "logging.info(f\"Genres after filtering:\\n{df_filtered['genre'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad0da7-0dc9-4259-a520-9890c9736fea",
   "metadata": {},
   "source": [
    "Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36589d55-7aa3-488a-9a80-3b6524a32e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X = tfidf.fit_transform(df_filtered['cleaned_plot'])\n",
    "y = df_filtered['genre']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fc727-51ca-416f-b43a-55025ec8c461",
   "metadata": {},
   "source": [
    "# 7.Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e8034bb-646d-47ac-a976-bdc78a1b2d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 09:43:49,779 - INFO - Total samples: 150\n",
      "2025-08-25 09:43:49,782 - INFO - Class distribution: Counter({np.int64(0): 50, np.int64(1): 50, np.int64(2): 50})\n",
      "2025-08-25 09:43:49,787 - INFO - Number of classes: 3\n",
      "2025-08-25 09:43:49,793 - INFO - Calculated test size: 0.20\n",
      "2025-08-25 09:43:49,803 - INFO - Training samples: 120, Test samples: 30\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris  # You can change this to load your own data\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Load dataset (Example: Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Check dataset stats\n",
    "total_samples = len(y)\n",
    "class_counts = Counter(y)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "logging.info(f\"Total samples: {total_samples}\")\n",
    "logging.info(f\"Class distribution: {class_counts}\")\n",
    "logging.info(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Calculate minimum valid test size for stratification\n",
    "min_test_size = num_classes / total_samples\n",
    "test_size = max(0.2, min_test_size)  # Ensure at least 20% test size\n",
    "\n",
    "logging.info(f\"Calculated test size: {test_size:.2f}\")\n",
    "\n",
    "# Split the dataset\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    logging.info(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "except ValueError as e:\n",
    "    logging.error(f\"Train-test split failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2862c-f88d-471c-95c8-e42738133318",
   "metadata": {},
   "source": [
    "# 8.Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0efc70aa-f6dd-4b95-ad95-ed21343243c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 09:45:14,114 - INFO - Total samples: 150\n",
      "2025-08-25 09:45:14,117 - INFO - Class distribution: Counter({np.int64(0): 50, np.int64(1): 50, np.int64(2): 50})\n",
      "2025-08-25 09:45:14,120 - INFO - Number of classes: 3\n",
      "2025-08-25 09:45:14,123 - INFO - Using test size: 0.20\n",
      "2025-08-25 09:45:14,127 - INFO - Training samples: 120\n",
      "2025-08-25 09:45:14,129 - INFO - Test samples: 30\n",
      "2025-08-25 09:45:14,131 - INFO - Class distribution in training set: {np.int64(0): 40, np.int64(2): 40, np.int64(1): 40}\n",
      "2025-08-25 09:45:14,132 - INFO - Computed class weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(0): 1.0, np.int64(1): 1.0, np.int64(2): 1.0}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris  # You can replace this with your own dataset\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ===== Load your dataset =====\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# ===== Analyze dataset =====\n",
    "total_samples = len(y)\n",
    "class_counts = Counter(y)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "logging.info(f\"Total samples: {total_samples}\")\n",
    "logging.info(f\"Class distribution: {class_counts}\")\n",
    "logging.info(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# ===== Calculate test size =====\n",
    "min_test_size = num_classes / total_samples\n",
    "default_test_size = 0.2\n",
    "test_size = max(default_test_size, min_test_size)\n",
    "\n",
    "logging.info(f\"Using test size: {test_size:.2f}\")\n",
    "\n",
    "# ===== Train-test split =====\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Training samples: {X_train.shape[0]}\")\n",
    "    logging.info(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "    # ===== Compute class weights =====\n",
    "    counter = Counter(y_train)\n",
    "    total = sum(counter.values())\n",
    "    class_weights = {\n",
    "        cls: total / (len(counter) * count) for cls, count in counter.items()\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Class distribution in training set: {dict(counter)}\")\n",
    "    logging.info(\"Computed class weights:\")\n",
    "    pprint.pprint(class_weights)\n",
    "\n",
    "except ValueError as e:\n",
    "    logging.error(f\"Error during split or class weight calculation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7d3ff-265b-4a8b-b5c5-55ee5db1b223",
   "metadata": {},
   "source": [
    "# 9.Train Logistic Regression with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b59981fa-05b9-4621-90cc-b30257022365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 18:14:05,684 - INFO - Number of training samples: 4\n",
      "2025-08-24 18:14:05,687 - INFO - Using 4-fold cross-validation\n",
      "2025-08-24 18:14:05,688 - INFO - Starting GridSearchCV...\n",
      "2025-08-24 18:14:05,701 - ERROR - Error during model training or hyperparameter tuning: n_splits=4 cannot be greater than the number of members in each class.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming X_train, y_train, and class_weights are already defined before this\n",
    "\n",
    "# Setup logging (if not already set)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "try:\n",
    "    # Log the size of the training set\n",
    "    n_train_samples = X_train.shape[0]\n",
    "    logging.info(f\"Number of training samples: {n_train_samples}\")\n",
    "\n",
    "    # Initialize LogisticRegression with computed class weights\n",
    "    base_model = LogisticRegression(max_iter=1000, class_weight=class_weights)\n",
    "\n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    }\n",
    "\n",
    "    # Adjust number of CV folds to be at most the number of training samples\n",
    "    cv_folds = min(5, n_train_samples)\n",
    "    if cv_folds < 2:\n",
    "        raise ValueError(\"Not enough training samples to perform cross-validation.\")\n",
    "\n",
    "    logging.info(f\"Using {cv_folds}-fold cross-validation\")\n",
    "\n",
    "    # Setup GridSearchCV\n",
    "    grid = GridSearchCV(base_model, param_grid, cv=cv_folds, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    logging.info(\"Starting GridSearchCV...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve and log best parameters\n",
    "    best_model = grid.best_estimator_\n",
    "    logging.info(f\"Best Parameters found: {grid.best_params_}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during model training or hyperparameter tuning: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cc65e-97a8-4da3-bca2-4bbb00a2214a",
   "metadata": {},
   "source": [
    "# 10.Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a54c9e2-85bf-4770-90b2-1ca4d3efea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 18:17:48,249 - INFO - Training samples: 120, Test samples: 30\n",
      "2025-08-24 18:17:48,251 - INFO - Class weights: {np.int64(0): 1.0, np.int64(2): 1.0, np.int64(1): 1.0}\n",
      "2025-08-24 18:17:48,253 - INFO - Starting GridSearchCV...\n",
      "2025-08-24 18:18:02,205 - INFO - Best Parameters found: {'C': 10, 'solver': 'lbfgs'}\n",
      "2025-08-24 18:18:02,211 - INFO - Test Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load dataset (replace with your data)\n",
    "        data = load_iris()\n",
    "        X, y = data.data, data.target\n",
    "\n",
    "        # Split dataset into train and test sets with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        logging.info(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "        # Calculate class weights to handle imbalanced classes\n",
    "        counter = Counter(y_train)\n",
    "        total = sum(counter.values())\n",
    "        class_weights = {cls: total / (len(counter) * count) for cls, count in counter.items()}\n",
    "        logging.info(f\"Class weights: {class_weights}\")\n",
    "\n",
    "        # Define base Logistic Regression model with class weights\n",
    "        base_model = LogisticRegression(max_iter=1000, class_weight=class_weights)\n",
    "\n",
    "        # Hyperparameter grid for tuning\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "\n",
    "        # Setup GridSearchCV\n",
    "        grid = GridSearchCV(base_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        logging.info(\"Starting GridSearchCV...\")\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        # Best model and parameters\n",
    "        best_model = grid.best_estimator_\n",
    "        logging.info(f\"Best Parameters found: {grid.best_params_}\")\n",
    "\n",
    "        # Prediction on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluation\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        logging.info(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3346a8-132a-4358-9441-3c06276aa916",
   "metadata": {},
   "source": [
    "# 11.Save Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5966702-914c-459c-8030-923436e89ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 18:28:51,568 - INFO - Saving trained model...\n",
      "2025-08-24 18:28:51,573 - INFO - Model saved successfully as 'movie_genre_clf.joblib'.\n",
      "2025-08-24 18:28:51,574 - INFO - Saving TF-IDF vectorizer...\n",
      "2025-08-24 18:28:51,578 - INFO - Vectorizer saved successfully as 'movie_genre_tfidf.joblib'.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def save_model_and_vectorizer(best_model=None, tfidf=None):\n",
    "    try:\n",
    "        if best_model is not None:\n",
    "            logging.info(\"Saving trained model...\")\n",
    "            joblib.dump(best_model, 'movie_genre_clf.joblib')\n",
    "            logging.info(\"Model saved successfully as 'movie_genre_clf.joblib'.\")\n",
    "        else:\n",
    "            logging.error(\"best_model is not defined. Please train the model before saving.\")\n",
    "            # Optionally, raise an exception or call your training function here\n",
    "            # raise ValueError(\"Model not trained\")\n",
    "\n",
    "        if tfidf is not None:\n",
    "            logging.info(\"Saving TF-IDF vectorizer...\")\n",
    "            joblib.dump(tfidf, 'movie_genre_tfidf.joblib')\n",
    "            logging.info(\"Vectorizer saved successfully as 'movie_genre_tfidf.joblib'.\")\n",
    "        else:\n",
    "            logging.warning(\"tfidf vectorizer not found. Skipping saving vectorizer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while saving model or vectorizer: {type(e).__name__}: {e}\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dummy example objects\n",
    "best_model = LogisticRegression()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Now call the function with these objects\n",
    "save_model_and_vectorizer(best_model=best_model, tfidf=tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904571c4-76d6-4ff5-ab9d-78a33f033f7b",
   "metadata": {},
   "source": [
    "# 12.Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "74794497-25e3-4655-9725-1101def811ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 18:35:35,918 - INFO - Model and vectorizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "def train_and_save_model():\n",
    "    plots = [\n",
    "        \"A team of explorers travel through a wormhole in space to save humanity.\",\n",
    "        \"A detective investigates a murder in a small town.\",\n",
    "        \"A love story between two young people in New York.\",\n",
    "        \"A spaceship crew tries to survive after an alien attack.\"\n",
    "    ]\n",
    "    genres = ['Sci-Fi', 'Mystery', 'Romance', 'Sci-Fi']\n",
    "\n",
    "    plots_cleaned = [clean_text(p) for p in plots]\n",
    "\n",
    "    # Remove stratify for small dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        plots_cleaned, genres, test_size=0.25, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "    best_model = LogisticRegression(max_iter=1000)\n",
    "    best_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    joblib.dump(tfidf, 'movie_genre_tfidf.joblib')\n",
    "    joblib.dump(best_model, 'movie_genre_clf.joblib')\n",
    "\n",
    "    logging.info(\"Model and vectorizer saved successfully.\")\n",
    "\n",
    "# Call the function to see the output\n",
    "train_and_save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da62cfd-4d8d-4fd5-9b02-7efd93c1c89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a5ef8-d994-4aae-89d5-5bff09603016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
